import express from 'express';
import http from 'http'; // Added for WebSocket server
import { WebSocketServer } from 'ws'; // Added for WebSocket server
import OpenAI from 'openai';
import { v4 as uuidv4 } from 'uuid';
import dotenv from "dotenv";
import cors from 'cors'; // Added for CORS

dotenv.config();
const app = express();
const port = process.env.PORT || 3000;

// --- Middlewares ---
app.use(cors()); // ADDED: Enable CORS for all HTTP routes
app.use(express.json());

const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
});

const sessions = new Map(); // Stores session history and now, the latest doctor summary

// --- System Prompt for the Care AI (Unchanged from your previous version) ---
const CARE_SYSTEM_PROMPT = `
You are "Care AI," an advanced, empathetic, and highly intelligent virtual health assistant for the "Care" mobile application.
Your primary role is to engage in supportive, multi-turn conversations with users about their health and well-being.
You are a crucial part of a larger ecosystem that includes a smartwatch for sensor data and a web application for doctors.

Your Core Directives:
1.  **Be Empathetic and Supportive:** Always adopt a caring, understanding, and patient tone. Health can be a sensitive topic.
2.  **Be Medically Cautious:** You are NOT a doctor. You cannot provide diagnoses, prescribe medication, or offer definitive medical advice. Always defer to qualified medical professionals for such matters. You can provide general health information and suggestions based on user input and available data.
3.  **Prioritize User Safety:** If a user describes symptoms that could indicate a serious or emergency situation, strongly advise them to seek immediate medical attention from emergency services or a doctor.
4.  **Clarify Ambiguity:** If a user's query is unclear, ask clarifying questions to ensure you understand their needs before providing information or using tools.
5.  **Maintain Context:** Remember previous turns in the conversation to provide relevant and coherent responses.
6.  **Use Available Tools Wisely:** You have access to tools that can provide real-time sensor data, user health records, and GPS location. Use them ONLY when necessary to answer a user's query or assess a situation more accurately. Always explain briefly why you are accessing certain data if it's not obvious.
7.  **Conciseness and Relevance:** Keep your responses focused on the user's current query or concern. Avoid overly long messages.
8.  **Summarization (User-Facing & Doctor Prep):**
    *   At appropriate points during the conversation, or if it seems to be concluding a topic, you can offer to summarize the key points for the user.
    *   Crucially, if you assess that the conversation contains significant information that a doctor should review (due to symptom severity, persistence, user concern, abnormal data, etc.), you should use the 'request_doctor_summary_generation' tool. You can also verbally inform the user that you think a summary for their doctor would be a good idea.
9.  **Escalation Awareness (Leading to Summary Request):** Based on the conversation and any data retrieved, if you determine the user's situation warrants review by their doctor, use the 'request_doctor_summary_generation' tool. Inform the user that you are flagging this for a more detailed summary which could be useful for their doctor.
10. **Privacy First:** Remind users that you are an AI and that they should be mindful of the information they share. All data is handled according to strict privacy protocols.

Tool Usage Guidelines:

You have access to the following tools. Only use them if the user's query explicitly or implicitly requires information these tools can provide.

*   **get_telemetry_data**: Retrieves recent physiological data (heart rate, SpO2) from the user's connected smartwatch/wristband.
*   **get_user_health_data**: Fetches specific health information (diagnoses, allergies, medications, age, weight, height) about the user from their secure profile.
*   **get_current_location**: Obtains the user's current GPS coordinates from their mobile device.

*   **request_doctor_summary_generation**:
    *   **Purpose**: To flag that the current conversation has reached a point where a detailed summary and an AI assessment for a doctor should be generated by the system. This is a backend process you are initiating.
    *   **Arguments**:
        *   \`userId\` (string, mandatory): The ID of the user. (Your backend will now ensure this is the current session's userId for this tool).
        *   \`reason\` (string, mandatory): A brief reason why you believe a summary for the doctor is now warranted (e.g., "User reported persistent chest pain for 3 days," "Telemetry data showed consistently high heart rate during reported anxiety," "User expressed significant concern about new symptoms and requested information to share with doctor," "Concluding discussion on managing a chronic condition flare-up.").
    *   **When to Use**: When you, Care AI, assess that the conversation contains significant information that a doctor should review. This might be due to the nature of symptoms, data from other tools, user's expressed anxiety, or if the user is concluding the discussion of a significant health issue. Use this tool call *in addition* to your textual response to the user. You should still verbally communicate to the user that you think a summary for their doctor might be helpful or is being prepared.
    *   **Example Invocation**: After a user describes new, concerning symptoms, and you've gathered some details, you would formulate your textual response to the user and also decide to call this tool with an appropriate reason.

Interaction Flow with Tools: (Standard flow: user message -> AI decides tool -> system calls tool -> tool result to AI -> AI final response)
Emergency Protocol Awareness:
*   The user's smartwatch can detect critical situations like faints or strokes.
*   In such events, a separate AI agent (not you, but part of the Care system) analyzes sensor data and can directly contact emergency services via speech-to-speech if necessary.
*   If a user mentions such an event or seems to be in distress, your role is to advise them to call emergency services immediately or confirm if help is on the way. You can ask if their smartwatch has triggered an alert.

Example of Escalation Suggestion (now tied to the tool):
User: "I've had this dull ache in my chest for three days now, and my heart rate seems a bit high according to my watch."
You (after potentially using get_telemetry_data, in your textual response): "I understand your concern about the persistent chest ache and your heart rate. Given these symptoms, I think it would be very helpful to prepare a summary of our conversation for your doctor. I'll flag this now. Please ensure you discuss this with your doctor soon."
(Internally, you would also call \`request_doctor_summary_generation\` with the current session's userId and a reason: "User reports persistent chest pain for three days and elevated heart rate concerns.")

Remember, your goal is to be a helpful, safe, and informative assistant.
Always prioritize the user's well-being and operate within your defined capabilities.
Let's begin.
`;


// --- traceStep function (Unchanged) ---
const traceStep = (passedTraceId, stepName, metadata = {}) => {
    const currentTraceId = passedTraceId || 'TRACE_ID_MISSING_' + uuidv4();
    console.log(`[Trace ID: ${currentTraceId}] [Step: ${stepName}]`, JSON.stringify(metadata));
};

// --- mockLangSmithMiddleware (Unchanged, applies to HTTP routes) ---
const mockLangSmithMiddleware = (req, res, next) => {
    req.traceId = uuidv4();
    traceStep(req.traceId, `http_request_starting`, {method: req.method, url: req.originalUrl, ip: req.ip});
    next();
};
app.use(mockLangSmithMiddleware);

// --- Tool Definitions for OpenAI (Unchanged) ---
const tools = [
    { type: "function", function: { name: "get_telemetry_data", description: "Retrieves recent physiological data (heart rate, SpO2).", parameters: { type: "object", properties: { userId: { type: "string", description: "User ID"}}, required: ["userId"]}}},
    { type: "function", function: { name: "get_user_health_data", description: "Fetches specific health information (diagnoses, allergies, etc.).", parameters: { type: "object", properties: { userId: { type: "string", description: "User ID"}}, required: ["userId"]}}},
    { type: "function", function: { name: "get_current_location", description: "Obtains the user's current GPS coordinates.", parameters: { type: "object", properties: { userId: { type: "string", description: "User ID"}}, required: ["userId"]}}},
    { type: "function", function: { name: "request_doctor_summary_generation", description: "Flags that a detailed summary for a doctor should be generated based on the conversation.", parameters: { type: "object", properties: { userId: { type: "string", description: "The ID of the user (will be overridden by session user ID)." }, reason: { type: "string", description: "Brief reason why a summary for the doctor is warranted." }}, required: ["userId", "reason"]}}},
];

// --- Mock Tool Implementations (Unchanged, ensure they take traceId) ---
async function get_telemetry_data(userId, traceId) {
    traceStep(traceId, "get_telemetry_data_called", { userId });
    return JSON.stringify({ avgHR: Math.floor(Math.random() * 20) + 65, maxHR: Math.floor(Math.random() * 30) + 90, minHR: Math.floor(Math.random() * 10) + 55, avgSpo2: Math.floor(Math.random() * 5) + 95, timestamp: new Date().toISOString() });
}
async function get_user_health_data(userId, traceId) {
    traceStep(traceId, "get_user_health_data_called", { userId });
    const mockUserData = { user123: { diagnosis: ["Mild Asthma"], allergies: ["Pollen"], medications: ["Albuterol Inhaler"], age: 30, weightKg: 70, heightCm: 175 }, default: { diagnosis: ["N/A"], allergies: ["None known"], medications: ["None"], age: "N/A" }};
    return JSON.stringify(mockUserData[userId] || mockUserData.default);
}
async function get_current_location(userId, traceId) {
    traceStep(traceId, "get_current_location_called", { userId });
    return JSON.stringify({ latitude: parseFloat((Math.random() * 180 - 90).toFixed(4)), longitude: parseFloat((Math.random() * 360 - 180).toFixed(4)), accuracy: Math.floor(Math.random() * 50) + 10, timestamp: new Date().toISOString() });
}
async function request_doctor_summary_generation(userId, reason, conversationHistory, traceId) {
    // userId here is now guaranteed to be the session's userId due to the fix in processChatMessage
    traceStep(traceId, "request_doctor_summary_generation_tool_called", { userId, reason });
    generateAndLogDoctorSummary(userId, conversationHistory, reason, traceId)
        .catch(error => {
            console.error(`[Trace ID: ${traceId}] Critical Error: Promise rejection from generateAndLogDoctorSummary for userId ${userId}:`, error.message);
            traceStep(traceId, "background_summary_promise_rejection", { userId, error: error.message });
        });
    return JSON.stringify({ status: "Doctor summary generation has been initiated.", reason_received: reason });
}
const availableTools = { get_telemetry_data, get_user_health_data, get_current_location, request_doctor_summary_generation };

// --- Doctor Summary Generation Function (With added logging - Unchanged from previous debug version) ---
async function generateAndLogDoctorSummary(userId, conversationHistory, reasonFromAI, traceId) {
    traceStep(traceId, "generate_doctor_summary_started", { userId, reasonFromAI, historyLength: conversationHistory.length });
    const summarizationPrompt = `
You are a specialized AI assistant tasked with creating a clinical summary from a patient-AI conversation for a human doctor.
Conversation History (last 10 relevant messages, system messages excluded for brevity unless critical):
${JSON.stringify(conversationHistory.filter(m => m.role === 'user' || m.role === 'assistant').slice(-10), null, 2)}
Based ONLY on the provided conversation history, please:
1.  summaryForDoctor: (string) Concise, structured summary (complaints, symptoms, data, concerns).
2.  aiPotentialAssessment: (string) "AI Potential Assessment (not a diagnosis): Possible [general issue category]. Medical evaluation needed."
3.  recommendEscalation: (boolean) True/false for doctor review.
4.  escalationReason: (string) Brief reason for escalation recommendation.
Return your response as a single, minified JSON object with these keys.
Original AI reason for this request: ${reasonFromAI}
    `;
    try {
        traceStep(traceId, "generate_doctor_summary_calling_openai", { userId });
        const summaryCompletion = await openai.chat.completions.create({
            model: "gpt-4o",
            messages: [{ role: "system", content: "You are a medical summarization assistant designed to output ONLY a valid JSON object based on the user's request." },{ role: "user", content: summarizationPrompt }],
            temperature: 0.2,
            max_tokens: 800,
            response_format: { type: "json_object" },
        });
        const summaryResultString = summaryCompletion.choices[0].message.content;
        traceStep(traceId, "doctor_summary_openai_responded", { userId, responseLength: summaryResultString?.length });

        let parsedSummary;
        try {
            parsedSummary = JSON.parse(summaryResultString);
            parsedSummary.originalAiReasonForSummary = reasonFromAI;
            parsedSummary.summaryGeneratedAt = new Date().toISOString();
            traceStep(traceId, "doctor_summary_parsed_successfully", { userId, keys: Object.keys(parsedSummary) });

        } catch (e) {
            console.error(`[Trace ID: ${traceId}] Could not parse summary JSON from model for userId ${userId}:`, e.message, "Raw Response:", summaryResultString);
            traceStep(traceId, "doctor_summary_parsing_failed", { userId, error: e.message, rawSubstr: summaryResultString?.substring(0,200) });
            parsedSummary = {
                error: "Failed to parse summary JSON from AI model.",
                details: e.message,
                rawResponseSnapshot: summaryResultString?.substring(0, 500),
                originalAiReasonForSummary: reasonFromAI,
                summaryGeneratedAt: new Date().toISOString()
            };
        }

        if (sessions.has(userId)) {
            const sessionData = sessions.get(userId);
            sessionData.latestDoctorSummary = parsedSummary;
            console.log(`[Trace ID: ${traceId}] DEBUG: Stored summary object for userId: ${userId}. Object keys: ${parsedSummary ? Object.keys(parsedSummary).join(', ') : 'null/undefined'}. Is error: ${!!parsedSummary?.error}`);
            traceStep(traceId, "doctor_summary_stored_in_session", { userId, hasErrorProperty: !!parsedSummary?.error });
        } else {
            console.error(`[Trace ID: ${traceId}] CRITICAL: Session not found for userId ${userId} when trying to store doctor summary.`);
            traceStep(traceId, "doctor_summary_not_stored_session_missing", { userId });
        }

        console.log(`\n--- [Trace ID: ${traceId}] DOCTOR SUMMARY FOR USER: ${userId} (Generated at ${parsedSummary.summaryGeneratedAt}) ---`);
        console.log(`AI's Reason for Request: ${reasonFromAI}`);
        console.log(JSON.stringify(parsedSummary, null, 2));
        console.log("--- END OF DOCTOR SUMMARY ---\n");

    } catch (error) {
        console.error(`[Trace ID: ${traceId}] Outer error in generateAndLogDoctorSummary for userId ${userId}:`, error.message, error.stack);
        traceStep(traceId, "doctor_summary_generation_outer_error", { userId, error: error.message });
        if (sessions.has(userId)) {
            const sessionData = sessions.get(userId);
            sessionData.latestDoctorSummary = {
                error: `Summary generation process failed: ${error.message}`,
                originalAiReasonForSummary: reasonFromAI,
                summaryGeneratedAt: new Date().toISOString()
            };
            console.log(`[Trace ID: ${traceId}] DEBUG: Stored error summary object for userId: ${userId} due to outer error.`);
            traceStep(traceId, "doctor_summary_error_stored_in_session", { userId });
        }
    }
}


// --- Core Chat Processing Logic (FIX APPLIED HERE) ---
async function processChatMessage(userId, message, traceId) { // 'userId' is the active session's userId
    traceStep(traceId, 'process_chat_message_started', { userId, messageContent: message?.substring(0, 100) + "..." });

    if (!sessions.has(userId)) {
        console.log(`[Trace ID: ${traceId}] DEBUG: Creating new session for userId: ${userId} in processChatMessage.`);
        traceStep(traceId, 'new_session_created_for_processing', { userId });
        sessions.set(userId, { history: [{ role: 'system', content: CARE_SYSTEM_PROMPT }], latestDoctorSummary: null });
    }

    const session = sessions.get(userId);
    if (!session) {
        console.error(`[Trace ID: ${traceId}] CRITICAL: Session object is null/undefined for userId ${userId} after attempting to get/set.`);
        return { text: "Critical server error: Session could not be established.", trace: [], usage: {} };
    }
    session.history.push({ role: 'user', content: message });
    traceStep(traceId, 'user_message_added_to_history_for_processing', { userId, count: session.history.length });

    let assistantResponsePayload = {
        text: "I'm sorry, an error occurred during processing your request.",
        trace: [{ step: 'processing_error_default' }],
        usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 }
    };

    try {
        traceStep(traceId, 'calling_openai_model_initial_processing', { userId, historyLength: session.history.length });
        let completion = await openai.chat.completions.create({
            model: "gpt-4o", messages: session.history, temperature: 0.7, max_tokens: 350, tools: tools, tool_choice: "auto",
        });
        traceStep(traceId, 'openai_model_responded_initial_processing', { userId, choiceType: completion.choices[0].message?.tool_calls ? 'tool_call' : 'text', toolCalls: completion.choices[0].message?.tool_calls?.map(tc=>tc.function.name) });

        let assistantMessage = completion.choices[0].message;
        assistantResponsePayload.usage = { ... (completion.usage || assistantResponsePayload.usage) };

        if (assistantMessage.tool_calls?.length > 0) {
            traceStep(traceId, 'tool_calls_requested_processing', { userId, calls: assistantMessage.tool_calls.map(tc => tc.function.name) });
            session.history.push(assistantMessage);

            for (const toolCall of assistantMessage.tool_calls) {
                const functionName = toolCall.function.name;
                const functionToCall = availableTools[functionName];
                const functionArgs = JSON.parse(toolCall.function.arguments);
                let functionResponse = JSON.stringify({error: `Tool ${functionName} not found or failed prior to execution.`});

                // ******** THE FIX IS HERE ********
                // For 'request_doctor_summary_generation', ALWAYS use the current session's 'userId'.
                // For other tools, the LLM might correctly specify a target if it were, e.g., a multi-user admin tool,
                // but for this app's context, summary is for the current user.
                const effectiveUserIdForTool = (functionName === "request_doctor_summary_generation") ? userId : (functionArgs.userId || userId);
                // ********************************

                traceStep(traceId, `executing_tool_${functionName}_for_user_${effectiveUserIdForTool}`, { args: functionArgs, originalSessionUserId: userId, llmProvidedUserIdInArgs: functionArgs.userId });

                if (functionToCall) {
                    try {
                        if (functionName === "request_doctor_summary_generation") {
                            // Pass the original session's userId (now in effectiveUserIdForTool)
                            functionResponse = await functionToCall(effectiveUserIdForTool, functionArgs.reason, session.history, traceId);
                        } else {
                            // For other tools, use effectiveUserIdForTool
                            functionResponse = await functionToCall(effectiveUserIdForTool, traceId);
                        }
                        traceStep(traceId, `tool_${functionName}_executed_successfully_processing`, { userId: effectiveUserIdForTool, responseSubstr: String(functionResponse).substring(0,70) });
                    } catch (toolError) {
                        console.error(`[Trace ID: ${traceId}] Tool ${functionName} (for user ${effectiveUserIdForTool}) error during processing:`, toolError.message, toolError.stack);
                        traceStep(traceId, `tool_${functionName}_execution_failed_processing`, { userId: effectiveUserIdForTool, error: toolError.message });
                        functionResponse = JSON.stringify({ error: "Tool execution failed", details: toolError.message });
                    }
                }
                session.history.push({ tool_call_id: toolCall.id, role: "tool", name: functionName, content: functionResponse });
            }

            traceStep(traceId, 'calling_openai_model_after_tools_processing', { userId, historyLength: session.history.length });
            const secondCompletion = await openai.chat.completions.create({
                model: "gpt-4o", messages: session.history, temperature: 0.7, max_tokens: 250,
            });
            assistantMessage = secondCompletion.choices[0].message;
            traceStep(traceId, 'openai_model_responded_after_tools_processing', { userId, hasContent: !!assistantMessage.content });
            if (secondCompletion.usage) {
                 assistantResponsePayload.usage.prompt_tokens += secondCompletion.usage.prompt_tokens;
                 assistantResponsePayload.usage.completion_tokens += secondCompletion.usage.completion_tokens;
                 assistantResponsePayload.usage.total_tokens += secondCompletion.usage.total_tokens;
            }
        }

        assistantResponsePayload.text = assistantMessage.content || "No textual content received from assistant in this turn.";
        session.history.push({ role: 'assistant', content: assistantResponsePayload.text });
        traceStep(traceId, 'assistant_message_added_to_history_processing', { userId, count: session.history.length });
        assistantResponsePayload.trace.push({ step: 'assistant_response_generated_processing' });

    } catch (error) {
        console.error(`[Trace ID: ${traceId}] Error in processChatMessage for userId ${userId}:`, error.message, error.stack);
        traceStep(traceId, 'process_chat_message_error', { userId, error: error.message });
        assistantResponsePayload.text = `An error occurred while processing your request: ${error.message}`;
    }
    return assistantResponsePayload;
}

// --- HTTP Routes (with added logging - Unchanged from previous debug version) ---
app.post('/chat', async (req, res) => {
    const { userId, message } = req.body;
    const traceId = req.traceId;

    console.log(`[Trace ID: ${traceId}] DEBUG HTTP POST /chat: Received userId: ${userId}, message: ${message?.substring(0,30)}...`);

    if (!userId || !message) {
        traceStep(traceId, 'http_chat_input_validation_failed', { reason: 'Missing userId or message' });
        return res.status(400).json({ text: "Missing userId or message.", trace: [], usage: {} });
    }
    const responsePayload = await processChatMessage(userId, message, traceId);
    res.json(responsePayload);
});

app.get('/chat/:userId/doctorsummary', (req, res) => {
    const { userId } = req.params;
    const traceId = req.traceId;

    console.log(`[Trace ID: ${traceId}] DEBUG HTTP GET /chat/${userId}/doctorsummary. Session exists: ${sessions.has(userId)}. Current session keys: ${Array.from(sessions.keys()).join(', ')}`);
    traceStep(traceId, 'http_doctorsummary_request_received', { userId, sessionExists: sessions.has(userId) });

    if (sessions.has(userId)) {
        const sessionData = sessions.get(userId);
        console.log(`[Trace ID: ${traceId}] DEBUG: For userId ${userId}, sessionData.latestDoctorSummary is ${sessionData.latestDoctorSummary ? 'PRESENT' : 'NULL or UNDEFINED'}.`);
        if (sessionData.latestDoctorSummary) {
            traceStep(traceId, 'http_doctorsummary_found_and_returned', { userId, summaryKeys: Object.keys(sessionData.latestDoctorSummary) });
            res.json(sessionData.latestDoctorSummary);
        } else {
            traceStep(traceId, 'http_doctorsummary_not_yet_available_in_session', { userId });
            res.status(404).json({ message: "Doctor summary not yet available." });
        }
    } else {
        traceStep(traceId, 'http_doctorsummary_user_session_not_found', { userId });
        res.status(404).json({ message: "User session not found for summary." });
    }
});


// --- WebSocket Server Setup (Unchanged from previous debug version) ---
const server = http.createServer(app);
const wss = new WebSocketServer({ server });

const activeClients = new Map();

wss.on('connection', (ws, req) => {
    const connectionTraceId = uuidv4();
    const clientId = uuidv4();
    activeClients.set(clientId, { ws, userId: null });

    traceStep(connectionTraceId, 'websocket_client_connected', { clientId, ip: req.socket.remoteAddress });
    ws.send(JSON.stringify({ type: 'system_message', text: 'Connected to Care AI. Please send init_connection with your userId.'}));

    ws.on('message', async (messageString) => {
        const messageProcessingTraceId = uuidv4();
        let parsedMessage;
        const clientData = activeClients.get(clientId);

        try {
            parsedMessage = JSON.parse(messageString);
            traceStep(messageProcessingTraceId, 'websocket_message_received', { clientId, associatedUserId: clientData?.userId, type: parsedMessage.type, msgUserId: parsedMessage.userId });

            if (parsedMessage.type === 'init_connection' && parsedMessage.userId) {
                if (clientData) {
                    clientData.userId = parsedMessage.userId;
                    console.log(`[Trace ID: ${messageProcessingTraceId}] DEBUG WS: Associated clientId ${clientId} with userId ${clientData.userId}`);
                    traceStep(messageProcessingTraceId, 'websocket_userId_associated', { clientId, userId: clientData.userId });
                    ws.send(JSON.stringify({ type: 'system_message', text: `Session initialized for user ${clientData.userId}.`}));
                } else {
                     traceStep(messageProcessingTraceId, 'websocket_init_error_client_not_found_in_activeClients', { clientId });
                     ws.send(JSON.stringify({ type: 'error', message: 'Internal client session error during initialization.' }));
                }
            } else if (parsedMessage.type === 'chat_message' && parsedMessage.message) {
                let currentUserId = clientData?.userId;

                if (!currentUserId && parsedMessage.userId) {
                    currentUserId = parsedMessage.userId;
                    if(clientData) clientData.userId = currentUserId;
                    console.log(`[Trace ID: ${messageProcessingTraceId}] DEBUG WS: Late association for clientId ${clientId} with userId ${currentUserId} from chat_message.`);
                    traceStep(messageProcessingTraceId, 'websocket_userId_late_association', { clientId, userId: currentUserId });
                } else if (!currentUserId && !parsedMessage.userId) {
                    traceStep(messageProcessingTraceId, 'websocket_chat_error_no_userid_available', { clientId });
                    ws.send(JSON.stringify({ type: 'error', message: 'User ID not provided or associated. Cannot process chat message.' }));
                    return;
                }

                const responsePayload = await processChatMessage(currentUserId, parsedMessage.message, messageProcessingTraceId);

                if (activeClients.has(clientId) && activeClients.get(clientId).ws.readyState === WebSocket.OPEN) {
                    activeClients.get(clientId).ws.send(JSON.stringify({
                        type: 'ai_response',
                        payload: responsePayload,
                        originalMessage: parsedMessage.message,
                        userId: currentUserId
                    }));
                    traceStep(messageProcessingTraceId, 'websocket_ai_response_sent', { clientId, userId: currentUserId });
                } else {
                    traceStep(messageProcessingTraceId, 'websocket_ai_response_not_sent_client_state_invalid', { clientId, userId: currentUserId, clientExists: activeClients.has(clientId), readyState: activeClients.get(clientId)?.ws.readyState });
                }
            } else {
                traceStep(messageProcessingTraceId, 'websocket_unknown_message_type_or_incomplete', { clientId, data: parsedMessage });
                ws.send(JSON.stringify({ type: 'error', message: 'Unknown message type or missing required data.'}));
            }
        } catch (error) {
            console.error(`[Trace ID: ${messageProcessingTraceId}] Error processing WebSocket message for clientId ${clientId} (User: ${clientData?.userId}):`, error.message, error.stack);
            traceStep(messageProcessingTraceId, 'websocket_message_processing_error', { clientId, userId: clientData?.userId, error: error.message });
            if (ws.readyState === ws.OPEN) {
                ws.send(JSON.stringify({ type: 'error', message: 'Error processing your message on the server.' }));
            }
        }
    });

    ws.on('close', () => {
        const clientData = activeClients.get(clientId);
        traceStep(connectionTraceId, 'websocket_client_disconnected', { clientId, userId: clientData?.userId, code: ws.protocolError && ws.protocolError.code, reason: ws.protocolError && ws.protocolError.reason });
        activeClients.delete(clientId);
    });

    ws.on('error', (error) => {
        const clientData = activeClients.get(clientId);
        console.error(`[Trace ID: ${connectionTraceId}] WebSocket error for client ${clientId} (User: ${clientData?.userId}):`, error.message);
        traceStep(connectionTraceId, 'websocket_client_error', { clientId, userId: clientData?.userId, error: error.message });
        activeClients.delete(clientId);
    });
});

server.listen(port, () => {
    console.log(`Care AI server (HTTP & WebSocket) listening at http://localhost:${port}`);
    console.log("OpenAI API Key Loaded:", !!process.env.OPENAI_API_KEY);
    console.log(`View sessions map directly (for debugging): global.sessions = sessions; global.activeClients = activeClients;`);
    global.sessions = sessions;
    global.activeClients = activeClients;
});