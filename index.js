import express from 'express';
import http from 'http'; // Added for WebSocket server
import { WebSocketServer } from 'ws'; // Added for WebSocket server
import OpenAI from 'openai';
import { v4 as uuidv4 } from 'uuid';
import dotenv from "dotenv";
import cors from 'cors'; // Added for CORS

dotenv.config();
const app = express();
const port = process.env.PORT || 3000;

// --- Middlewares ---
app.use(cors()); // ADDED: Enable CORS for all HTTP routes
app.use(express.json());

const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
});

const sessions = new Map(); // Stores session history and now, the latest doctor summary

// --- System Prompt for the Care AI (Unchanged from your previous version) ---
const CARE_SYSTEM_PROMPT = `
You are "Care AI," an advanced, empathetic, and highly intelligent virtual health assistant for the "Care" mobile application.
Your primary role is to engage in supportive, multi-turn conversations with users about their health and well-being.
You are a crucial part of a larger ecosystem that includes a smartwatch for sensor data and a web application for doctors.

Your Core Directives:
(Existing directives 1-7 remain the same)
8.  **Summarization (User-Facing & Doctor Prep):**
    *   At appropriate points during the conversation, or if it seems to be concluding a topic, you can offer to summarize the key points for the user.
    *   Crucially, if you assess that the conversation contains significant information that a doctor should review (due to symptom severity, persistence, user concern, abnormal data, etc.), you should use the 'request_doctor_summary_generation' tool. You can also verbally inform the user that you think a summary for their doctor would be a good idea.
9.  **Escalation Awareness (Leading to Summary Request):** Based on the conversation and any data retrieved, if you determine the user's situation warrants review by their doctor, use the 'request_doctor_summary_generation' tool. Inform the user that you are flagging this for a more detailed summary which could be useful for their doctor.
(Existing directive 10 remains the same)

Tool Usage Guidelines:

You have access to the following tools. Only use them if the user's query explicitly or implicitly requires information these tools can provide.

*   **get_telemetry_data**: (description remains the same)
*   **get_user_health_data**: (description remains the same)
*   **get_current_location**: (description remains the same)

*   **request_doctor_summary_generation**:
    *   **Purpose**: To flag that the current conversation has reached a point where a detailed summary and an AI assessment for a doctor should be generated by the system. This is a backend process you are initiating.
    *   **Arguments**:
        *   \`userId\` (string, mandatory): The ID of the user.
        *   \`reason\` (string, mandatory): A brief reason why you believe a summary for the doctor is now warranted (e.g., "User reported persistent chest pain for 3 days," "Telemetry data showed consistently high heart rate during reported anxiety," "User expressed significant concern about new symptoms and requested information to share with doctor," "Concluding discussion on managing a chronic condition flare-up.").
    *   **When to Use**: When you, Care AI, assess that the conversation contains significant information that a doctor should review. This might be due to the nature of symptoms, data from other tools, user's expressed anxiety, or if the user is concluding the discussion of a significant health issue. Use this tool call *in addition* to your textual response to the user. You should still verbally communicate to the user that you think a summary for their doctor might be helpful or is being prepared.
    *   **Example Invocation**: After a user describes new, concerning symptoms, and you've gathered some details, you would formulate your textual response to the user and also decide to call this tool with an appropriate reason.

Interaction Flow with Tools: (remains the same)
Emergency Protocol Awareness: (remains the same)
Example of Escalation Suggestion (now tied to the tool):
User: "I've had this dull ache in my chest for three days now, and my heart rate seems a bit high according to my watch."
You (after potentially using get_telemetry_data, in your textual response): "I understand your concern about the persistent chest ache and your heart rate. Given these symptoms, I think it would be very helpful to prepare a summary of our conversation for your doctor. I'll flag this now. Please ensure you discuss this with your doctor soon."
(Internally, you would also call \`request_doctor_summary_generation\` with userId and reason: "User reports persistent chest pain for three days and elevated heart rate concerns.")

Remember, your goal is to be a helpful, safe, and informative assistant.
Always prioritize the user's well-being and operate within your defined capabilities.
Let's begin.
`;


// --- traceStep function (Unchanged) ---
const traceStep = (passedTraceId, stepName, metadata = {}) => {
    const currentTraceId = passedTraceId || 'TRACE_ID_MISSING_' + uuidv4();
    console.log(`[Trace ID: ${currentTraceId}] [Step: ${stepName}]`, JSON.stringify(metadata));
};

// --- mockLangSmithMiddleware (Unchanged, applies to HTTP routes) ---
const mockLangSmithMiddleware = (req, res, next) => {
    req.traceId = uuidv4();
    traceStep(req.traceId, `http_request_starting`, {method: req.method, url: req.originalUrl});
    next();
};
app.use(mockLangSmithMiddleware); // This applies to HTTP routes defined *after* it.

// --- Tool Definitions for OpenAI (Unchanged) ---
const tools = [
    { type: "function", function: { name: "get_telemetry_data", description: "Retrieves recent physiological data...", parameters: { type: "object", properties: { userId: { type: "string"}}, required: ["userId"]}}},
    { type: "function", function: { name: "get_user_health_data", description: "Fetches specific health information...", parameters: { type: "object", properties: { userId: { type: "string"}}, required: ["userId"]}}},
    { type: "function", function: { name: "get_current_location", description: "Obtains the user's current GPS coordinates.", parameters: { type: "object", properties: { userId: { type: "string"}}, required: ["userId"]}}},
    { type: "function", function: { name: "request_doctor_summary_generation", description: "Flags that a detailed summary...", parameters: { type: "object", properties: { userId: { type: "string" }, reason: { type: "string" }}, required: ["userId", "reason"]}}},
];

// --- Mock Tool Implementations (Unchanged, ensure they take traceId) ---
async function get_telemetry_data(userId, traceId) {
    traceStep(traceId, "get_telemetry_data_called", { userId });
    return JSON.stringify({ avgHR: Math.floor(Math.random() * 20) + 65, maxHR: Math.floor(Math.random() * 30) + 90, minHR: Math.floor(Math.random() * 10) + 55, avgSpo2: Math.floor(Math.random() * 5) + 95, timestamp: new Date().toISOString() });
}
async function get_user_health_data(userId, traceId) {
    traceStep(traceId, "get_user_health_data_called", { userId });
    const mockUserData = { user123: { diagnosis: ["Mild Asthma"], allergies: ["Pollen"], medications: ["Albuterol Inhaler"], age: 30, weightKg: 70, heightCm: 175 }, default: { diagnosis: ["N/A"], allergies: [], medications: [], age: "N/A" }};
    return JSON.stringify(mockUserData[userId] || mockUserData.default);
}
async function get_current_location(userId, traceId) {
    traceStep(traceId, "get_current_location_called", { userId });
    return JSON.stringify({ latitude: parseFloat((Math.random() * 180 - 90).toFixed(4)), longitude: parseFloat((Math.random() * 360 - 180).toFixed(4)), accuracy: Math.floor(Math.random() * 50) + 10, timestamp: new Date().toISOString() });
}
async function request_doctor_summary_generation(userId, reason, conversationHistory, traceId) {
    traceStep(traceId, "request_doctor_summary_generation_tool_called", { userId, reason });
    generateAndLogDoctorSummary(userId, conversationHistory, reason, traceId)
        .catch(error => {
            console.error(`[Trace ID: ${traceId}] Error in background summary generation:`, error.message);
            traceStep(traceId, "background_summary_generation_error", { error: error.message });
        });
    return JSON.stringify({ status: "Doctor summary generation initiated.", reason_received: reason });
}
const availableTools = { get_telemetry_data, get_user_health_data, get_current_location, request_doctor_summary_generation };

// --- Doctor Summary Generation Function (Unchanged) ---
async function generateAndLogDoctorSummary(userId, conversationHistory, reasonFromAI, traceId) {
    traceStep(traceId, "generate_doctor_summary_started", { userId, reasonFromAI });
    const summarizationPrompt = `
You are a specialized AI assistant tasked with creating a clinical summary from a patient-AI conversation for a human doctor.
Conversation History (last 10 relevant messages):
${JSON.stringify(conversationHistory.filter(m => m.role === 'user' || m.role === 'assistant').slice(-10), null, 2)}
Based ONLY on the provided conversation history, please:
1.  summaryForDoctor: (string) Concise, structured summary (complaints, symptoms, data, concerns).
2.  aiPotentialAssessment: (string) "AI Potential Assessment (not a diagnosis): Possible [general issue category]. Medical evaluation needed."
3.  recommendEscalation: (boolean) True/false for doctor review.
4.  escalationReason: (string) Brief reason for escalation recommendation.
Return as a single, minified JSON object with these keys.
Original AI reason for this request: ${reasonFromAI}
    `;
    try {
        const summaryCompletion = await openai.chat.completions.create({
            model: "gpt-4o", messages: [{ role: "system", content: "You are a medical summarization assistant designed to output JSON." },{ role: "user", content: summarizationPrompt }],
            temperature: 0.2, max_tokens: 700, response_format: { type: "json_object" },
        });
        const summaryResultString = summaryCompletion.choices[0].message.content;
        traceStep(traceId, "doctor_summary_generated_raw_content", { length: summaryResultString?.length });

        let parsedSummary;
        try {
            parsedSummary = JSON.parse(summaryResultString);
            parsedSummary.originalAiReasonForSummary = reasonFromAI;
            parsedSummary.summaryGeneratedAt = new Date().toISOString();

            if (sessions.has(userId)) {
                const sessionData = sessions.get(userId);
                sessionData.latestDoctorSummary = parsedSummary;
                traceStep(traceId, "doctor_summary_stored_in_session", { userId });
            } else {
                traceStep(traceId, "doctor_summary_not_stored_session_missing", { userId });
            }
        } catch (e) {
            console.error(`[Trace ID: ${traceId}] Could not parse summary JSON from model:`, e.message, "Raw:", summaryResultString);
            traceStep(traceId, "doctor_summary_parsing_failed", { error: e.message, rawSubstr: summaryResultString?.substring(0,100) });
            parsedSummary = { error: "Failed to parse summary from AI.", raw: summaryResultString, originalAiReasonForSummary: reasonFromAI, summaryGeneratedAt: new Date().toISOString() };
            if (sessions.has(userId)) sessions.get(userId).latestDoctorSummary = parsedSummary;
        }
        console.log(`\n--- [Trace ID: ${traceId}] DOCTOR SUMMARY FOR USER: ${userId} ---`);
        console.log(JSON.stringify(parsedSummary, null, 2));
        console.log("--- END OF DOCTOR SUMMARY ---\n");
    } catch (error) {
        console.error(`[Trace ID: ${traceId}] Error generating doctor summary:`, error.message, error.stack);
        traceStep(traceId, "doctor_summary_generation_failed", { error: error.message });
        if (sessions.has(userId)) sessions.get(userId).latestDoctorSummary = { error: `Summary generation failed: ${error.message}`, originalAiReasonForSummary: reasonFromAI, summaryGeneratedAt: new Date().toISOString() };
    }
}


// --- Core Chat Processing Logic (to be reused by HTTP and WebSocket) ---
async function processChatMessage(userId, message, traceId) {
    traceStep(traceId, 'process_chat_message_started', { userId, messageContent: message?.substring(0, 50) + "..." });

    if (!sessions.has(userId)) {
        traceStep(traceId, 'new_session_created_for_processing', { userId });
        sessions.set(userId, { history: [{ role: 'system', content: CARE_SYSTEM_PROMPT }], latestDoctorSummary: null });
    }

    const session = sessions.get(userId);
    session.history.push({ role: 'user', content: message });
    traceStep(traceId, 'user_message_added_to_history_for_processing', { count: session.history.length });

    let assistantResponsePayload = {
        text: "I'm sorry, an error occurred during processing.",
        trace: [{ step: 'processing_error_default' }],
        usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 }
    };

    try {
        if (userId === 'user123' && session.history.filter(m => m.role === 'user').length === 3) { // Example error simulation
            traceStep(traceId, 'simulating_api_error_for_processing', { userId });
            throw new Error('Simulated OpenAI API timeout for user123 on 3rd message during processing.');
        }

        traceStep(traceId, 'calling_openai_model_initial_processing', { historyLength: session.history.length });
        let completion = await openai.chat.completions.create({
            model: "gpt-4o", messages: session.history, temperature: 0.7, max_tokens: 300, tools: tools, tool_choice: "auto",
        });
        traceStep(traceId, 'openai_model_responded_initial_processing', { choiceType: completion.choices[0].message?.tool_calls ? 'tool_call' : 'text' });

        let assistantMessage = completion.choices[0].message;
        assistantResponsePayload.usage = { ... (completion.usage || assistantResponsePayload.usage) };

        if (assistantMessage.tool_calls?.length > 0) {
            traceStep(traceId, 'tool_calls_requested_processing', { calls: assistantMessage.tool_calls.map(tc => tc.function.name) });
            session.history.push(assistantMessage);

            for (const toolCall of assistantMessage.tool_calls) {
                const functionName = toolCall.function.name;
                const functionToCall = availableTools[functionName];
                const functionArgs = JSON.parse(toolCall.function.arguments);
                let functionResponse = JSON.stringify({error: `Tool ${functionName} not found or failed prior to execution.`});

                if (functionToCall) {
                    try {
                        if (functionName === "request_doctor_summary_generation") {
                            functionResponse = await functionToCall(functionArgs.userId, functionArgs.reason, session.history, traceId);
                        } else {
                            functionResponse = await functionToCall(functionArgs.userId, traceId);
                        }
                        traceStep(traceId, `tool_${functionName}_executed_successfully_processing`, { responseSubstr: String(functionResponse).substring(0,70) });
                    } catch (toolError) {
                        console.error(`[Trace ID: ${traceId}] Tool ${functionName} error during processing:`, toolError.message);
                        traceStep(traceId, `tool_${functionName}_execution_failed_processing`, { error: toolError.message });
                        functionResponse = JSON.stringify({ error: "Tool execution failed", details: toolError.message });
                    }
                }
                session.history.push({ tool_call_id: toolCall.id, role: "tool", name: functionName, content: functionResponse });
            }

            traceStep(traceId, 'calling_openai_model_after_tools_processing', { historyLength: session.history.length });
            const secondCompletion = await openai.chat.completions.create({
                model: "gpt-4o", messages: session.history, temperature: 0.7, max_tokens: 250,
            });
            assistantMessage = secondCompletion.choices[0].message;
            if (secondCompletion.usage) {
                 assistantResponsePayload.usage.prompt_tokens += secondCompletion.usage.prompt_tokens;
                 assistantResponsePayload.usage.completion_tokens += secondCompletion.usage.completion_tokens;
                 assistantResponsePayload.usage.total_tokens += secondCompletion.usage.total_tokens;
            }
        }

        assistantResponsePayload.text = assistantMessage.content || "No textual content received from assistant.";
        session.history.push({ role: 'assistant', content: assistantResponsePayload.text });
        traceStep(traceId, 'assistant_message_added_to_history_processing', { count: session.history.length });
        assistantResponsePayload.trace.push({ step: 'assistant_response_generated_processing' });

    } catch (error) {
        console.error(`[Trace ID: ${traceId}] Error in processChatMessage:`, error.message, error.stack);
        traceStep(traceId, 'process_chat_message_error', { error: error.message });
        assistantResponsePayload.text = `An error occurred: ${error.message}`; // Keep it simple for client
    }
    return assistantResponsePayload;
}

// --- HTTP Routes (Unchanged) ---
app.post('/chat', async (req, res) => {
    const { userId, message } = req.body;
    const traceId = req.traceId; // From mockLangSmithMiddleware

    if (!userId || !message) {
        traceStep(traceId, 'http_chat_input_validation_failed', { reason: 'Missing userId or message' });
        return res.status(400).json({ text: "Missing userId or message.", trace: [], usage: {} });
    }
    const responsePayload = await processChatMessage(userId, message, traceId);
    res.json(responsePayload);
});

app.get('/chat/:userId/doctorsummary', (req, res) => {
    const { userId } = req.params;
    const traceId = req.traceId; // From mockLangSmithMiddleware
    traceStep(traceId, 'http_doctorsummary_request_received', { userId });
    if (sessions.has(userId)) {
        const sessionData = sessions.get(userId);
        if (sessionData.latestDoctorSummary) {
            res.json(sessionData.latestDoctorSummary);
        } else {
            res.status(404).json({ message: "Doctor summary not yet available." });
        }
    } else {
        res.status(404).json({ message: "User session not found." });
    }
});


// --- ADDITIONS: HTTP Server and WebSocket Server Setup ---
const server = http.createServer(app); // Use Express app to handle HTTP requests
const wss = new WebSocketServer({ server }); // Attach WebSocket server to the HTTP server

const activeClients = new Map(); // To keep track of connected WebSocket clients and their userIds

wss.on('connection', (ws, req) => {
    const connectionTraceId = uuidv4(); // Trace ID for this WebSocket connection's lifecycle
    const clientId = uuidv4(); // Unique ID for this WebSocket client instance
    activeClients.set(clientId, { ws }); // Store the WebSocket connection

    traceStep(connectionTraceId, 'websocket_client_connected', { clientId, ip: req.socket.remoteAddress });
    ws.send(JSON.stringify({ type: 'system_message', text: 'Connected to Care AI via WebSocket.'}));

    ws.on('message', async (messageString) => {
        const messageProcessingTraceId = uuidv4(); // Trace ID for processing this specific message
        let parsedMessage;
        try {
            parsedMessage = JSON.parse(messageString);
            const clientData = activeClients.get(clientId);
            traceStep(messageProcessingTraceId, 'websocket_message_received', { clientId, currentUserId: clientData?.userId, type: parsedMessage.type, receivedUserId: parsedMessage.userId });

            if (parsedMessage.type === 'init_connection' && parsedMessage.userId) {
                if (clientData) {
                    clientData.userId = parsedMessage.userId; // Associate userId with this WebSocket connection
                    activeClients.set(clientId, clientData); // Update client data
                    traceStep(messageProcessingTraceId, 'websocket_userId_associated', { clientId, userId: clientData.userId });
                    ws.send(JSON.stringify({ type: 'system_message', text: `Session initialized for user ${clientData.userId}.`}));
                } else {
                     traceStep(messageProcessingTraceId, 'websocket_init_error_client_not_found', { clientId });
                     ws.send(JSON.stringify({ type: 'error', message: 'Client session error during initialization.' }));
                }
            } else if (parsedMessage.type === 'chat_message' && parsedMessage.message) {
                const currentUserId = clientData?.userId || parsedMessage.userId; // Prefer associated userId, fallback to message's userId
                if (!currentUserId) {
                    traceStep(messageProcessingTraceId, 'websocket_chat_error_no_userid', { clientId });
                    ws.send(JSON.stringify({ type: 'error', message: 'User ID not associated with this session. Please send init_connection first or include userId.' }));
                    return;
                }
                 // Ensure the clientData has the most up-to-date userId if it was just provided
                if (clientData && !clientData.userId && parsedMessage.userId) {
                    clientData.userId = parsedMessage.userId;
                    activeClients.set(clientId, clientData);
                }


                const responsePayload = await processChatMessage(currentUserId, parsedMessage.message, messageProcessingTraceId);

                if (activeClients.has(clientId)) { // Check if client is still connected
                    activeClients.get(clientId).ws.send(JSON.stringify({
                        type: 'ai_response',
                        payload: responsePayload,
                        originalMessage: parsedMessage.message, // Echo back original for context if needed
                        userId: currentUserId
                    }));
                    traceStep(messageProcessingTraceId, 'websocket_ai_response_sent', { clientId, userId: currentUserId });
                } else {
                    traceStep(messageProcessingTraceId, 'websocket_ai_response_not_sent_client_disconnected', { clientId, userId: currentUserId });
                }
            } else {
                traceStep(messageProcessingTraceId, 'websocket_unknown_message_type', { clientId, data: parsedMessage });
                ws.send(JSON.stringify({ type: 'error', message: 'Unknown message type or missing required data.'}));
            }
        } catch (error) {
            console.error(`[Trace ID: ${messageProcessingTraceId}] Error processing WebSocket message:`, error);
            traceStep(messageProcessingTraceId, 'websocket_message_processing_error', { clientId, error: error.message });
            if (ws.readyState === ws.OPEN) { // Check if ws is still open before sending
                ws.send(JSON.stringify({ type: 'error', message: 'Error processing your message on the server.' }));
            }
        }
    });

    ws.on('close', () => {
        const clientData = activeClients.get(clientId);
        traceStep(connectionTraceId, 'websocket_client_disconnected', { clientId, userId: clientData?.userId });
        activeClients.delete(clientId);
    });

    ws.on('error', (error) => {
        const clientData = activeClients.get(clientId);
        console.error(`[Trace ID: ${connectionTraceId}] WebSocket error for client ${clientId} (User: ${clientData?.userId}):`, error);
        traceStep(connectionTraceId, 'websocket_client_error', { clientId, userId: clientData?.userId, error: error.message });
        activeClients.delete(clientId); // Ensure client is removed on error
    });
});
// --- END OF ADDITIONS ---


// MODIFIED: Start the HTTP server (which also hosts the WebSocket server)
server.listen(port, () => {
    console.log(`Care AI server (HTTP & WebSocket) listening at http://localhost:${port}`);
    console.log("OpenAI API Key Loaded:", !!process.env.OPENAI_API_KEY);
});